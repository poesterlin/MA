\chapter{Results from the Literature Research}

There is not much research focused on alternative locomotion techniques in VR that do not require controllers. It is a new and experimental field of study.

\section{Tracking Technology}
Locomotion systems using hand-tracking traditionally required external tracking devices like a leap motion sensor mounted to the VR headset developed by the company Ultraleap \cite{Ultraleap}.
This works for a prototype but can not be expected to be something an end-user is able to set up. The studies conducted using the leap motion sensor also reported users complaining about the sensors field of view like in the study conducted by Schäfer et al. \cite{Schafer2021}.
The sensor has a 140° field of view and therefore can see the environment in front of the headset well, however it is not able to track hands if they are held below the headset or next to the users' body, where it would be more natural and ergonomic. A much more usable implementation of hand-tracking is implemented by the Oculus Quest headsets. It does not require any cables or additional hardware. The headset is internally processing the images of up to 4 cameras that are able to detect the users' hands using a trained neural network developed by Han et al \cite{Han}.
Even though the cameras only produce black and white images and sometimes only one camera is able to see the hand, the tracking works remarkably well. There are however major limitations since the detection network is not trained to detect hand-to-hand interactions and therefore cuts out entirely until the user separates their hands again. The tracking is also sometimes inaccurate if some fingers are obstructed by others. In most cases this is not very visible to the user since the fingers are also obstructed from their perspective, it is however visible in the tracking information recorded by the headset and has to be taken into account. 

\section{Requirements for Teleportation Systems}
Enabling simulated movement in virtual environments is not a new problem. The techniques that allow the user to move their avatars viewpoint can be implemented in a lot of different ways that all can have different effects on different people. Therefore they require special attention in regards to many different factors. Otherwise moving can result in motion sickness, a low level of immersions, disorientation and an overall frustrating user experience. To produce a good VR experience, it is important to understand the users needs when it comes to locomotion and motion sickness. 

Motion sickness or more accurately cybersickness is a phenomenon not completely understood. It can be a problem in VR like it is when riding the train backwards, reading a book in a moving car and even for pilots in airplane simulators. VR should be fun and cause as little cybersickness as possible. To reduce the effects there are a couple of techniques. First, the application should perform well and run with a consistently high framerate. Performance is a key consideration when developing for VR. Modern hardware makes this less and less of a problem and it is possible to run even highly detailed applications with a high framerate. However, cybersickness will sill come up for some people if there is optical flow when moving in VR while standing still in real life. Optical flow is the phenomenon that allows the brain to gather information about the motion of objects the give the user a sense of presence. This effect produces an immersive VR experience if it does not stand in conflict with the sensors the human body has that are not tricked by a VR headset. Optical flow during movement in VR is especially a problem in the peripheral vision. To reduce this some applications allow the user to enable a virtual helmet that reduces the field of view. A good example of this would be the game Rollercoaster Simulator. A rollercoaster ride moves the player continuously in VR while they are standing still in real life. This can make even experienced VR users motion sick, so the implementation of the helmet makes a lot of sense. 

The most popular way to implement locomotion is to allow the user to teleport in the virtual environment. A teleportation jump instantaneously transports the avatars viewpoint to a new locomotion without any transition. This way the effects of motion sickness are reduced since there is no optical flow. No optical flow is a good way to reduce cybersickness, however it also has an effect on the immersiveness of the experience. Some users have a harder time experiencing the environment fully since the optical flow is also a major factor controlling the level of immersion. This is not ideal however it is not as sirius of an effect as some users not able to use the application at all because of the cybersickness symptoms. 

In traditional controller-based applications the focus lied on the implementation itself and activating the locomotion only required a button press. With hand-tracking, there is another human factors to consider, namely the ergonomics of repeating some action over potentially multiple hours. Ergonomics can be something different from person to person, there are some key rules to follow though. This includes allowing the hands to held at a natural hight, not requiring the elbow to be flexed to more than 90 degrees over a long time, not requiring hands to turn more than 45 degrees from the position of the palm facing the ground and allowing movement to mostly come out of the elbows and shoulders. 


\section{Gestures}
The literature has come up with a handful of gestures and ways to study them in a laboratory environment. The games industry however has implemented some fully usable systems for actual users. Both types of sources were taken into account to produce this list of comparable gestures:

\begin{itemize}
    \item One-Handed Palm Gesture by Schäfer et al. \cite{Schafer2021}:
    Researchers propose a teleportation gesture that is using a single, wide-open hand. The target location is selected using a ray starting as the normal vector of the palm of the hand. The teleportation is then executed after 1.5 seconds.

    \item One-Handed Index Gesture by Schäfer et al. \cite{Schafer2021}: 
    Researchers propose a teleportation gesture that is using one hand with the index finger extended to use as a pointing device. To select a point, a user can just point at it. The teleportation is then executed after 1.5 seconds.
    
    \item Two-Handed Palm Gesture by Schäfer et al. \cite{Schafer2021}: 
    Same targeting gesture as the One-Handed Index Gesture. The second hand is used to confirm the teleport by opening the hand.

    \item One-Handed Index Gesture by Schäfer et al. \cite{Schafer2021}: 
    Same targeting gesture as the One-Handed Index Gesture. The second hand is used to confirm the teleport by extending the index finger.

    \item Triangle Gesture found in the game Elixir by Magnopus \cite{Magnopus}:
    A game called Elixir which is a short hand-tracking demo for the Oculus Quest uses a bimanual gesture. Both hands form a triangle connecting both thumbs and index fingers. This can be used to target a point on the floor. To confirm the target location, the user pinches thumbs and index fingers together. 
    
    \item Pull Gesture found in the game Vacation Simulator by Owlchemy Labs \cite{VacSimOculus}:
    A game called Vacation Simulator has one of the best implementations of hand-tracking found in games so far. The locomotion is using a one-handed pulling gesture. In the game, this is only used for low-resolution target selection.

    \item Palm-steering Gesture by Caggianese et al. \cite{Caggianese}:
    This method is using the normal vector of the open palm to set a target. Once a target has been found it is locked as soon as the user closes their hand to a fist. The movement is then started and the user will be moved to the target over a period of time.
    However, since this gesture is using continuous movement and not teleportation and therefore is not really comparable to the others.
    
    \item Index-steering Gesture by Caggianese et al. \cite{Caggianese}:
    To target a point in space using this gesture, a vector is created from the headsets position through the tip of the users extended index finger. To start the movement the user has to extend their thumb as a confirmation.
    However, this gesture is using continuous movement and not teleportation and therefore is not really comparable to the others.
\end{itemize}

After my own subjective testing using a prototype of the final application, the systems using a time delay were found to be disorienting. Because the teleportation can not be confirmed explicitly it is not transparent to the user when it is going to be executed. The locomotion systems of the first four gestures are also very similar and lead to very similar results in the previous testing. Also the similarity to the palm- and index-steering gestures proposed by Caggianese et al. would make studying the two methods separately kind of redundant. Because of this, it was decided to combine the one- and two-handed systems. This results in two one-handed systems that have a confirmation step to execute the teleportation. To get feedback on how intuitive this change was, a gesture elicitation study was performed. The pull gesture was not implemented since it is only used for low-resolution target selection in the previous implementation. The triangle gesture is not described in the literature so it was be replicated as closely as possible. There could however still be small differences in the way the implementation works which might not represent the original gesture exactly. After the selection the final three gestures are:

% TODO: detail
Index Gesture: 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/indexinfo.jpg}
    \caption{The two steps of the index gesture}
    \label{fig:indexInfo}
\end{figure}

Palm Gesture:
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/palminfo.jpg}
    \caption{The two steps of the palm gesture}
    \label{fig:palmInfo}
\end{figure}

Triangle Gesture:
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/triangleinfo.jpg}
    \caption{The two steps of the triangle gesture}
    \label{fig:triangleInfo}
\end{figure}

\subsection{Gesture Detection}
To detect if the current hand position is part of a gesture Schäfer et al. \cite{Schafer2021} propose an algorithm
that detects gestures based on the finger state and the palm direction. The finger state can only be curled or
stretched. The authors give the example of a fist gesture that is characterized by to have all fingers in a curled
state. For direction dependent gestures like a thumbs up gesture, the algorithm also takes the direction of the
palm into account. This seams to have some limitations for gestures like an okay sign where the thumb and index
finger are touching and are nether in a curled nor stretched state. However without the actual implementation that
Schäfer et al. developed its hard to know if the okay sign could be recognized or not.

\section{Testing Methodology}
The internal test game studios are using are not known, so the sources for the testing methodology are limited. In the literature, many studies rely on completion time and the number of errors during a task as a measure of a good locomotion system. Examples are Schäfer et al. \cite{Schafer2021} and Bozgeyikli et al \cite{bozgeyikli}.
This might cause some problems however since theoretically a system that can teleport right to the end of the environment would have a much better completion time than a system with a shorter teleport range even though this would not be a good system for the user. To combat this, it is necessary to have the same maximum teleport distance to compare systems. However, it would be better to not rely on the completion time as a measure and rather look for actual signs of user satisfaction. The number of errors seems like a good measure though since it can be very frustrating and disorienting to be teleported to the wrong location. 

The testing environments used for the studies are very diverse. Sometimes the prototype implementations are using a detailed environment so that it is very immersive for the test participant, like Caggianese et al. \cite{Caggianese} and other times it is created so that it is not distracting the player and therefore very clean and simple like Schäfer et al. \cite{Schafer2021}.  
Since there is no one-to-one comparison between results of the same locomotion system in a different environment, there is not an established, way to approach this.

The task the user is supposed to execute in the testing environment differs also depending on the study, however, mostly it is about teleporting to checkpoints along a corridor like Schäfer et al. \cite{Schafer2021} propose or in an open space.
This way the user is forced to hit points in the world accurately. This mechanic is easy to test however it is also not that similar to a game or a productivity application that an actual user would use. The need to hit an exact point and teleporting around just to get from one point to another without an objective is rarely found. Therefore adding an objective like avoiding an enemy or searching for something in the environment while teleporting would create a more realistic testing environment. This puts the teleportation itself in the background as it would be in an actual implementation of a teleportation system. However, if the system is not satisfying to use, the user will be able to notice the deficiencies since it is impeding the objective. An objective like this would make the task completion measure even more of a problem since it would not be the fault of the teleportation system if the user can not complete the objective on the first try and then takes longer. 

