\chapter{Custom Unity Code}

\section{Gesture Recognition System}
To recognize a gesture it has to be compared to a stored gesture. The stored gestures are downloaded from the server when the VR application is started. They can then be compared to the current hand position. This algorithm is adapted from a small script found on Github.  % TODO: cite https://github.com/jorgejgnz/HandTrackingGestureRecorder 
The gestures consists of joint positions and the algorithm compares each of them to the current positions relative to the base of the hand. If a joint is too far of and the distance exceeds a threshold the gesture is skipped. If all joints are within the threshold, all distances are added to a sum. The gesture that has the lowest total distance to the current hand position is then chosen as the recognized gesture. There is also a time based threshold that checks if the gesture is displayed by the user for at least the time set by the time threshold variable. This prevents accidental activation. The gestures that are part of a set and can currently not be reached are skipped without evaluation so they do not take up any processing time. Another optimization is to skip the evaluation of joints if one hand is not included in the gesture at all. Those optimizations are important because the algorithm is run for every frame and has to look at the 46 joint positions of all hands so it is important that it is as fast as possible.
If a hand is skipped, for the included joints there is a default distance assumed. This is because otherwise a one handed gesture would always be picked over a two handed gesture even if the two handed gesture was indented by the user. Due to the fact that there are always small differences in the tracking or the users hand that are summed up so, a bimanual gesture would always have a larger distance compared to a skipped hand with zero distance. 
The output of the gesture recognition system is the type of gesture currently displayed by the user, the index of the current gesture if its part of a set, including information for the teleport service about key positions. The positions are adapted to whatever hand the user prefers.

\section{Teleportation System}
The teleportation system is build in a modular fashion so that it is easy to extend. There is a Teleporter base class that contains most of the methods needed for all the various teleportation methods. If the gesture recognition system detects a new gesture, a new instance of the corresponding teleport class is then created. This is active as long as the gesture does not change or until a teleport is executed. Otherwise a cleanup method is called and the Teleporter instance is discarded. 

To be able to create a vector from the gesture, the positions of key joints are extracted from the current hand position. The Oculus framework uses an identifier for every joint as well as one for each finger tip and the base of the hand. The Visualizer can show them for debugging (see \ref{fig:fingerIds})

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth/5]{figures/fingerIds.jpg}
    \caption{The numbers of every joint.}
    \label{fig:fingerIds}
\end{figure}

% TODO: describe teleporters and left handed mode

If the teleporter has converted a users input gesture to a position in the environment using a ray cast, the location is picked up and a teleport is executed. This requires the deactivation of the Oculus player controller, so that a new position for the player and the camera can be set. After the teleport is complete, the player controller is enabled again. Other than the teleport, this process is invisible to the user. 

\section{Study Observers}
In each scene used for the study there is an observer script that is able to listen to teleports, completed tasks and collect statistics. This information is relayed to the server where it is stored for analysis.