\section{Custom Unity Code}

\subsection{Gesture Recognition System}
To recognize a gesture it has to be compared to a stored gesture. The stored gestures are downloaded from the server when the VR application is started. They can then be compared to the current hand position. This algorithm is adapted from a small script found on Github created by the user jorgejgnz \cite{github}.
The gestures consist of joint positions and the algorithm compares each of them to the current positions relative to the base of the hand. If a joint is too far of and the distance exceeds a threshold the gesture is skipped. If all joints are within the threshold, all distances are added to a sum. The gesture that has the lowest total distance to the current hand position is then chosen as the recognized gesture. There is also a time-based threshold that checks if the gesture is displayed by the user for at least the time set by the time threshold variable. This prevents accidental activation. The gestures that are part of a set and can currently not be reached are skipped without evaluation so they do not take up any processing time. Another optimization is to skip the evaluation of joints if one hand is not included in the gesture at all. Those optimizations are important because the algorithm is run for every frame and has to look at the 46 joint positions of all hands so it must be as fast as possible.
If a hand is skipped, for the included joints there is a default distance assumed. This is because otherwise a one-handed gesture would always be picked over a two-handed gesture even if the two-handed gesture was invented by the user. Due to the fact that there are always small differences in the tracking or the users' hand that are summed up so, a bimanual gesture would always have a larger distance compared to a skipped hand with zero distance. 
The output of the gesture recognition system is the type of gesture currently displayed by the user, the index of the current gesture if it is part of a set, including information for the teleport service about key positions. The positions are adapted to whatever hand the user prefers.

Inspired by Sch√§fer et al. \cite{Schafer2021}, the algorithm originally also took into account the palm orientation to be able to identify direction dependent gestures. However this was found to interfere with the targeting. If the user points at a close point, the gesture is oriented significancy further down than if they teleport to a point meters away. To make this work it would require a threshold of almost 90 degrees in some axes. That would not allow the palm orientation metric to help tell the three implemented gestures apart. That is why the final version of the algorithm does not include this. It could be added back in if there is a need for direction dependent gestures in the future possibly with options what axes to ignore in the evaluation for more customization. The palm direction is still stored and used for the Visualizer.

\subsection{Teleportation System}
The teleportation system is built in a modular fashion so that it is easy to extend. There is a Teleporter base class that contains most of the methods needed for all the various teleportation methods. If the gesture recognition system detects a new gesture, a new instance of the corresponding teleport class is then created. This is active as long as the gesture does not change or until a teleport is executed. Otherwise, a cleanup method is called and the Teleporter instance is discarded. 

To be able to create a vector from the gesture, the positions of key joints are extracted from the current hand position. The Oculus framework uses an identifier for every joint as well as one for each fingertip and the base of the hand. The Visualizer can show them for debugging (see \ref{fig:fingerIds})

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fingerIds.jpg}
    \caption{The numbers of every joint.}
    \label{fig:fingerIds}
\end{figure}

The three types of target selection methods all function using a ray cast. A ray cast is using a start position to find objects that can be found in the direction of a vector. The vectors the ray follows and the start positions are created differently for each method. 
The palm and the triangle gesture both use a plane created using different joints. The normal vector of the plane is used to get the direction of the ray cast. The start position is a joint in the middle of the hand, in the case of the palm gesture as seen in \ref{fig:palmTracker}. The triangle gesture is using the middle of the hands. It is calculated by linear interpolating from the base of one index finger halfway to the base of the index finger of the other hand. The tip of the triangle is calculated in a similar way using the tips of the index fingers as seen in \ref{fig:triangleTracker}. The selection method using the index gesture is generating a ray cast vector direction from the position of the wrist towards the position of the tip of the index finger as seen in \ref{fig:indexTracker}. All methods are using some amount of smoothing applied to the selection targets to make the output more resilient to tracking errors. This is done by linear interpolating the last target to the next target by a small amount. This way the new target only contributes a limited amount of change. The smoothing values are adapted to fit the accuracy of each gesture. The palm gesture for example requires more smoothing since the confirmation step can otherwise disturb the previously set target.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/palm tracker.jpg}
    \caption{The joints used to construct and plane from the palm gesture.}
    \label{fig:palmTracker}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/triangle tracker.jpg}
    \caption{The joints used to construct and plane from the triangle gesture.}
    \label{fig:triangleTracker}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/index tracker.jpg}
    \caption{The joints used to construct a vector from the index gesture.}
    \label{fig:indexTracker}
\end{figure}

If the teleporter has converted a users input gesture to a position in the environment using a ray cast, the location is picked up and a teleport is executed. This requires the deactivation of the Oculus player controller so that a new position for the player and the camera can be set. After the teleport is complete, the player controller is enabled again. Other than the teleport, this process is invisible to the user. 

\subsection{Study Observers}
In each scene used for the study, there is an observer script that is able to listen to teleports, complete tasks and collect statistics. This information is relayed to the server where it is stored for analysis.