\chapter{Design of Teleportation Techniques}
\label{cha:ImplementationOverview}

% TODO new intro
The custom prototype implemented for the comparative user study was built to be able to get all gestures into the same, controlled VR environment. The implementation was done using the Unity game engine. More technical information can be found in the master report. 

\section{Implemented Gestures}

To keep the implementation and study time in a reasonable timeframe, only a limited number of locomotion systems can be picked for the comparative study. Also, to keep the systems comparable, only teleportation based systems are going to be considered since teleportation is currently the most popular and accessible method. This leaves the gestures from the games industry and the gestures researched by Sch채fer et al. \cite{Schafer2021}. 

The games industry has produced gesture systems for a variety of environments and uses cases. The vacation simulator pull gesture is only used for low-resolution tasks and is therefore too limited. The telepath system is very interesting but harder to compare to the other systems. This would have made it difficult to implement and evaluate. %cite
The elixir gesture system, on the other hand, will be implemented for the study since it was also proposed by the participants of the Gesture Evaluation Study.  %cite 
This system will be referred to as the triangle gesture in the following work. As seen in figure \ref{fig:triangleTracker}, the gesture is converted to a vector by creating a plane from the base of the two thumbs and the center point of the tips of the index fingers. The normal vector of this plane is used to select the target destination.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/triangle tracker.jpg}
    \caption{The joints used to construct and plane from the triangle gesture.}
    \label{fig:triangleTracker}
\end{figure}

The gestures from the Sch채fer study have variations for both two and one-handed modes. The second hand is only used to confirm the teleport and not for the targeting system. 
This is interesting when compared to the study performed by Caggianese et al. \cite{Caggianese} where the same one-handed gestures are used only with a confirmation method using the same hand. Sch채fer et al. use a time delay of $1.5$ seconds to confirm the one-handed versions of their gestures. According to the participants of the Gesture Evaluation Study, this might lead to unexpected teleport jumps, and a confirmation step would be preferred. The confirmation mechanic developed by Caggianese et al. is proven to work well and could make the one-handed index and palm methods of Sch채fer more controllable. The gestures were therefore modified in this way for the implementation to reduce user discomfort and comply with the users' intuitions better. The resulting gestures are referred to as index gestures and palm gestures, respectively. 

Aiming using the index gesture is done using the vector from the base of the hand to the tip of the index finger, as seen in figure \ref{fig:indexTracker} projected out into the environment. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/index tracker.jpg}
    \caption{The joints used to construct a vector from the index gesture.}
    \label{fig:indexTracker}
\end{figure}

The palm gesture is converted into a vector by creating a plane from three points, as shown in figure \ref{fig:palmTracker}. The wrist position and the tip of the index and ring finger are used to create the plane. The normal vector of the plane is used for the targeting.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/palm tracker.jpg}
    \caption{The joints used to construct and plane from the palm gesture.}
    \label{fig:palmTracker}
\end{figure}

\section{Requirements}
The most important requirement for the Unity implementation was to be able to provide a comfortable experience for the study participants. For this, the application needed to run at over 70 frames per second, even on the limited hardware of the Quest 2 device, to eliminate a common source of cybersickness. Second, the gesture recognition algorithm had to recognize the gestures accurately and quickly. The approach taken to optimize the detection as well as the performance is explained in this chapter. The third requirement was to make the system as self-explanatory as possible in order to make the study go smoothly and consistently. 

\section{Performance}
To be able complete the first requires some technical optimizations were done to make the environments for the task as performant as necessary. Additionally, the detailed forest environment was built using easy to render elements with low polygon counts. This way, there was still performance budged left over, and the most expensive hand-tracking settings could be used for improved responsiveness and accuracy.

\section{Calibration}
A big problem early on in the development of the detection algorithm was caused by how different human hands can be. In order to fulfil the second requirement, the detection had to be accurate enough to decide between stored gestures based only on one finger being in a different position while staying flexible enough so a user could adjust their hands' posture or simply have their own interpretation of the gesture. This turned out to be quite difficult to implement. The final implementation took the following approaches to improve the detection accuracy and speed.

In order to make the detection more universally accurate to all hand sizes, the system first measures the size of the user's hand. This is achieved by adding up the distance from the wrist joint up to the tip of the middle finger. It was found that this is an accurate enough and very consistent method. The output value of the size calibration allows the detection system to scale the joint positions up or down depending on the user's hand size. This improves the accuracy of the detection. In the current implementation, this can not be done universally for every user, however. In some cases, the distances between the gesture states are simply too small to be able to handle the differences in anatomy or execution-style that are still present after the size calibration. This leads to inaccurate detections early on in testing, which is frustrating for the user to experience. Therefore an explicit calibration step was added that would record the users' input for every gesture state before it can be used in order to optimize the detection. The recorded ground truth is stored in the system and is used together with previously-stored joint positions in order to be able to detect the gesture state accurately. The size calibration also helps in this case since the positions are all on a universal scale. The previously stored variants are first tested if they would lead to a wrong detection when compared to the ground truth and filtered out automatically if they are not compatible with the current user. This improves the accuracy, keeps the calibration short and provides the user with a high degree of flexibility since they can rely on the calibration input of other, compatible users if they do not execute a gesture change perfectly similar to the calibration run or just naturally shift the way they are gesturing during use.

\section{Gesture Recognition}
A gesture system has multiple assigned states mapped to different actions like aiming and confirming a teleport location. In the current use case, the states are not completely different, however. Depending on the gesture system, there is sometimes a lot of overlap between the states. This allows for some optimization in the algorithm. The detection system first compares the joint positions of the joints that are the same in all states. A distance to the stored base version of the gesture system is computed and compared to a threshold to be able to tell if the user is currently presenting a gesture. This threshold was set after a lot of experimentation but can also be changed dynamically if there should be any problems with the detection. After the system has determined if the static joints are in the right position for a gesture, the next step is to analyze what state the gesture is in. For this, the system runs through all variants that are available after the calibration step. The variant with the smallest distance to the tracked gesture is recorded. The distance needs to be under a second threshold for the variant to be considered. This threshold is calculated per user during the calibration step based on the ground truth recordings. If a variant passes all checks, it is first stored locally inside the recognition system until it is recognized consistently for $0.5$ seconds. After this last check is completed, the detection system uses the information included in the variants data about what state it is assigned to and passes that information on. It was found during testing that this time threshold filters out false-positive recognitions effectively while still allowing for fast trigger times.


\section{Teleport Mechanic and Design}
In order to complete the third requirement, various design decisions were taken to guide the user. In addition to textual information about the current task, virtual hands would show the user how to use the gesture and how it should be held so the teleportation arc would show up. An example of this can be seen in figure \ref{fig:tutorial}. They are only shown until the user figures out what to do and manages to complete a teleportation step and get out of the way so the user can complete their task.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/tutorialhands.png}
    \caption{Animated rendered hands to show the user how to perform the gesture and where the teleportation arc is create from.}
    \label{fig:tutorial}
\end{figure}

During the presentation of the previous results in an expert interview, the question came up about how to improve the user experience of the targeting system that would convert a vector from the gesture into a teleport destination. In the games industry, it is standard practice to use a ray-cast of some kind and a target reticle that shows the destination point. The ray-cast can be implemented linearly or by using a simulated projectile path. Both have some benefits and drawbacks, especially in this use case since it might force the user to put their hands in an uncomfortable position to reach a target. It was proposed that the system should adapt to the user. This means that if the vector produced from the gesture recognition system hits a valid point in the world, this point will be used as a destination, as shown in figure \ref{fig:tpray}. However, if the vector does not hit the ground within a specified distance, therefore producing an invalid result, the targeting system uses a simulated projectile path to curve the teleport path downwards, as shown in figure \ref{fig:tparc}. Both targeting versions have the same smoothing applied in order to eliminate jitter caused by small tracking inconsistencies or gesture changes. A small confirmation sound was also added to give the user additional feedback when they are teleporting. This was also a good addition for debugging since it is easily apparent that a teleport was not performed when there is no confirmation sound, even without looking at the users' screen.

\begin{figure}[!htb]
    \minipage{0.40\textwidth}
        \includegraphics[width=\linewidth]{figures/teleport ray.png}
        \caption{Teleport ray originating from triangle gesture with reticle.}
        \label{fig:tpray}
    \endminipage\hfill
    \minipage{0.40\textwidth}
        \includegraphics[width=\linewidth]{figures/teleportArc.png}
        \caption{Curved teleport arc originating from triangle gesture with reticle.}
        \label{fig:tparc}
    \endminipage\hfill
\end{figure}